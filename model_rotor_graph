digraph {
	graph [size="127.64999999999999,127.64999999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1665458185744 [label="
 (8, 1, 16384)" fillcolor=darkolivegreen1]
	1665458955280 [label=TanhBackward0]
	1665458954272 -> 1665458955280
	1665458954272 [label=ConvolutionBackward0]
	1665458953696 -> 1665458954272
	1665458953696 [label=CatBackward0]
	1665458955760 -> 1665458953696
	1665458955760 [label=LeakyReluBackward1]
	1665458959312 -> 1665458955760
	1665458959312 [label=CudnnBatchNormBackward0]
	1665458951056 -> 1665458959312
	1665458951056 [label=ConvolutionBackward0]
	1665458959888 -> 1665458951056
	1665458959888 [label=CatBackward0]
	1665458961040 -> 1665458959888
	1665458961040 [label=UpsampleLinear1DBackward1]
	1665458955136 -> 1665458961040
	1665458955136 [label=LeakyReluBackward1]
	1665458954896 -> 1665458955136
	1665458954896 [label=CudnnBatchNormBackward0]
	1665458957536 -> 1665458954896
	1665458957536 [label=ConvolutionBackward0]
	1665458949616 -> 1665458957536
	1665458949616 [label=CatBackward0]
	1665458956480 -> 1665458949616
	1665458956480 [label=UpsampleLinear1DBackward1]
	1665458958160 -> 1665458956480
	1665458958160 [label=LeakyReluBackward1]
	1665458956288 -> 1665458958160
	1665458956288 [label=CudnnBatchNormBackward0]
	1665458946640 -> 1665458956288
	1665458946640 [label=ConvolutionBackward0]
	1665458952496 -> 1665458946640
	1665458952496 [label=CatBackward0]
	1665458954512 -> 1665458952496
	1665458954512 [label=UpsampleLinear1DBackward1]
	1665458951728 -> 1665458954512
	1665458951728 [label=LeakyReluBackward1]
	1665458951104 -> 1665458951728
	1665458951104 [label=CudnnBatchNormBackward0]
	1665458949712 -> 1665458951104
	1665458949712 [label=ConvolutionBackward0]
	1665458950048 -> 1665458949712
	1665458950048 [label=CatBackward0]
	1665458951584 -> 1665458950048
	1665458951584 [label=UpsampleLinear1DBackward1]
	1665458958064 -> 1665458951584
	1665458958064 [label=LeakyReluBackward1]
	1665458957056 -> 1665458958064
	1665458957056 [label=CudnnBatchNormBackward0]
	1665458950288 -> 1665458957056
	1665458950288 [label=ConvolutionBackward0]
	1665458955376 -> 1665458950288
	1665458955376 [label=CatBackward0]
	1665458961568 -> 1665458955376
	1665458961568 [label=UpsampleLinear1DBackward1]
	1665458961808 -> 1665458961568
	1665458961808 [label=LeakyReluBackward1]
	1665458953312 -> 1665458961808
	1665458953312 [label=CudnnBatchNormBackward0]
	1665458946976 -> 1665458953312
	1665458946976 [label=ConvolutionBackward0]
	1665458959984 -> 1665458946976
	1665458959984 [label=CatBackward0]
	1665458961088 -> 1665458959984
	1665458961088 [label=UpsampleLinear1DBackward1]
	1665458956048 -> 1665458961088
	1665458956048 [label=LeakyReluBackward1]
	1665458961760 -> 1665458956048
	1665458961760 [label=CudnnBatchNormBackward0]
	1665458958016 -> 1665458961760
	1665458958016 [label=ConvolutionBackward0]
	1665458953168 -> 1665458958016
	1665458953168 [label=CatBackward0]
	1665458952352 -> 1665458953168
	1665458952352 [label=UpsampleLinear1DBackward1]
	1665458946208 -> 1665458952352
	1665458946208 [label=LeakyReluBackward1]
	1665458953744 -> 1665458946208
	1665458953744 [label=CudnnBatchNormBackward0]
	1665458949088 -> 1665458953744
	1665458949088 [label=ConvolutionBackward0]
	1665458948896 -> 1665458949088
	1665458948896 [label=CatBackward0]
	1665458956864 -> 1665458948896
	1665458956864 [label=UpsampleLinear1DBackward1]
	1665458958592 -> 1665458956864
	1665458958592 [label=LeakyReluBackward1]
	1665458958688 -> 1665458958592
	1665458958688 [label=CudnnBatchNormBackward0]
	1665458955040 -> 1665458958688
	1665458955040 [label=ConvolutionBackward0]
	1665458960944 -> 1665458955040
	1665458960944 [label=CatBackward0]
	1665458955568 -> 1665458960944
	1665458955568 [label=UpsampleLinear1DBackward1]
	1665458956240 -> 1665458955568
	1665458956240 [label=LeakyReluBackward1]
	1665458952880 -> 1665458956240
	1665458952880 [label=CudnnBatchNormBackward0]
	1665458951008 -> 1665458952880
	1665458951008 [label=ConvolutionBackward0]
	1665458952976 -> 1665458951008
	1665458952976 [label=CatBackward0]
	1665458953600 -> 1665458952976
	1665458953600 [label=UpsampleLinear1DBackward1]
	1665458950768 -> 1665458953600
	1665458950768 [label=LeakyReluBackward1]
	1665458946448 -> 1665458950768
	1665458946448 [label=CudnnBatchNormBackward0]
	1665458952784 -> 1665458946448
	1665458952784 [label=ConvolutionBackward0]
	1665458951968 -> 1665458952784
	1665458951968 [label=CatBackward0]
	1665458956096 -> 1665458951968
	1665458956096 [label=UpsampleLinear1DBackward1]
	1665458949328 -> 1665458956096
	1665458949328 [label=LeakyReluBackward1]
	1665458948512 -> 1665458949328
	1665458948512 [label=CudnnBatchNormBackward0]
	1665458946304 -> 1665458948512
	1665458946304 [label=ConvolutionBackward0]
	1665458946544 -> 1665458946304
	1665458946544 [label=CatBackward0]
	1665458954176 -> 1665458946544
	1665458954176 [label=UpsampleLinear1DBackward1]
	1665458946688 -> 1665458954176
	1665458946688 [label=LeakyReluBackward1]
	1665458948560 -> 1665458946688
	1665458948560 [label=CudnnBatchNormBackward0]
	1665458948224 -> 1665458948560
	1665458948224 [label=ConvolutionBackward0]
	1665458947984 -> 1665458948224
	1665458947984 [label=SliceBackward0]
	1665458946496 -> 1665458947984
	1665458946496 [label=SliceBackward0]
	1665458947120 -> 1665458946496
	1665458947120 [label=SliceBackward0]
	1665458947264 -> 1665458947120
	1665458947264 [label=LeakyReluBackward0]
	1665458948464 -> 1665458947264
	1665458948464 [label=CudnnBatchNormBackward0]
	1665458948128 -> 1665458948464
	1665458948128 [label=ConvolutionBackward0]
	1665458952208 -> 1665458948128
	1665458952208 [label=SliceBackward0]
	1665458950624 -> 1665458952208
	1665458950624 [label=SliceBackward0]
	1665458948320 -> 1665458950624
	1665458948320 [label=SliceBackward0]
	1665458947072 -> 1665458948320
	1665458947072 [label=LeakyReluBackward0]
	1665458960704 -> 1665458947072
	1665458960704 [label=CudnnBatchNormBackward0]
	1665458960272 -> 1665458960704
	1665458960272 [label=ConvolutionBackward0]
	1665458946160 -> 1665458960272
	1665458946160 [label=SliceBackward0]
	1665458951200 -> 1665458946160
	1665458951200 [label=SliceBackward0]
	1665458948416 -> 1665458951200
	1665458948416 [label=SliceBackward0]
	1665458952064 -> 1665458948416
	1665458952064 [label=LeakyReluBackward0]
	1665458957296 -> 1665458952064
	1665458957296 [label=CudnnBatchNormBackward0]
	1665458956384 -> 1665458957296
	1665458956384 [label=ConvolutionBackward0]
	1665458957008 -> 1665458956384
	1665458957008 [label=SliceBackward0]
	1665458947216 -> 1665458957008
	1665458947216 [label=SliceBackward0]
	1665458950864 -> 1665458947216
	1665458950864 [label=SliceBackward0]
	1665458961616 -> 1665458950864
	1665458961616 [label=AddBackward0]
	1665458949376 -> 1665458961616
	1665458949376 [label=LeakyReluBackward0]
	1665458960896 -> 1665458949376
	1665458960896 [label=CudnnBatchNormBackward0]
	1665458961424 -> 1665458960896
	1665458961424 [label=ConvolutionBackward0]
	1665458948992 -> 1665458961424
	1665458948992 [label=SliceBackward0]
	1665458960032 -> 1665458948992
	1665458960032 [label=SliceBackward0]
	1665458949184 -> 1665458960032
	1665458949184 [label=SliceBackward0]
	1665458959168 -> 1665458949184
	1665458959168 [label=LeakyReluBackward0]
	1665458962048 -> 1665458959168
	1665458962048 [label=CudnnBatchNormBackward0]
	1665458960128 -> 1665458962048
	1665458960128 [label=ConvolutionBackward0]
	1665458962192 -> 1665458960128
	1665458962192 [label=SliceBackward0]
	1665458959360 -> 1665458962192
	1665458959360 [label=SliceBackward0]
	1665458947552 -> 1665458959360
	1665458947552 [label=SliceBackward0]
	1665458949280 -> 1665458947552
	1665458949280 [label=LeakyReluBackward0]
	1665458958784 -> 1665458949280
	1665458958784 [label=CudnnBatchNormBackward0]
	1665458950096 -> 1665458958784
	1665458950096 [label=ConvolutionBackward0]
	1665458961664 -> 1665458950096
	1665458961664 [label=SliceBackward0]
	1665458952448 -> 1665458961664
	1665458952448 [label=SliceBackward0]
	1665458959408 -> 1665458952448
	1665458959408 [label=SliceBackward0]
	1665458956000 -> 1665458959408
	1665458956000 [label=LeakyReluBackward0]
	1665458958256 -> 1665458956000
	1665458958256 [label=CudnnBatchNormBackward0]
	1665458954944 -> 1665458958256
	1665458954944 [label=ConvolutionBackward0]
	1665458957152 -> 1665458954944
	1665458957152 [label=SliceBackward0]
	1665458954416 -> 1665458957152
	1665458954416 [label=SliceBackward0]
	1665458948944 -> 1665458954416
	1665458948944 [label=SliceBackward0]
	1665458956912 -> 1665458948944
	1665458956912 [label=AddBackward0]
	1665458950480 -> 1665458956912
	1665458950480 [label=LeakyReluBackward0]
	1666593961632 -> 1665458950480
	1666593961632 [label=CudnnBatchNormBackward0]
	1666593953664 -> 1666593961632
	1666593953664 [label=ConvolutionBackward0]
	1666593959424 -> 1666593953664
	1666593959424 [label=SliceBackward0]
	1666593952512 -> 1666593959424
	1666593952512 [label=SliceBackward0]
	1666593960864 -> 1666593952512
	1666593960864 [label=SliceBackward0]
	1665458957776 -> 1666593960864
	1665458957776 [label=LeakyReluBackward0]
	1666593954336 -> 1665458957776
	1666593954336 [label=CudnnBatchNormBackward0]
	1666593955152 -> 1666593954336
	1666593955152 [label=ConvolutionBackward0]
	1666593961056 -> 1666593955152
	1666593961056 [label=SliceBackward0]
	1666593952176 -> 1666593961056
	1666593952176 [label=SliceBackward0]
	1666593960144 -> 1666593952176
	1666593960144 [label=SliceBackward0]
	1665458952640 -> 1666593960144
	1665458952640 [label=LeakyReluBackward0]
	1666593952656 -> 1665458952640
	1666593952656 [label=CudnnBatchNormBackward0]
	1666593950064 -> 1666593952656
	1666593950064 [label=ConvolutionBackward0]
	1666593957456 -> 1666593950064
	1666593957456 [label=SliceBackward0]
	1666593954192 -> 1666593957456
	1666593954192 [label=SliceBackward0]
	1666593948384 -> 1666593954192
	1666593948384 [label=SliceBackward0]
	1665458954608 -> 1666593948384
	1665458954608 [label=LeakyReluBackward0]
	1666593950400 -> 1665458954608
	1666593950400 [label=CudnnBatchNormBackward0]
	1666593957648 -> 1666593950400
	1666593957648 [label=ConvolutionBackward0]
	1666593951264 -> 1666593957648
	1666593951264 [label=SliceBackward0]
	1666593954624 -> 1666593951264
	1666593954624 [label=SliceBackward0]
	1666593954432 -> 1666593954624
	1666593954432 [label=SliceBackward0]
	1665458956672 -> 1666593954432
	1665458956672 [label=AddBackward0]
	1666593960192 -> 1665458956672
	1666593960192 [label=LeakyReluBackward0]
	1666593960576 -> 1666593960192
	1666593960576 [label=CudnnBatchNormBackward0]
	1666593953328 -> 1666593960576
	1666593953328 [label=ConvolutionBackward0]
	1666594550304 -> 1666593953328
	1661311720624 [label="encoder.0.main.0.weight
 (24, 1, 15)" fillcolor=lightblue]
	1661311720624 -> 1666594550304
	1666594550304 [label=AccumulateGrad]
	1666594550400 -> 1666593953328
	1661311720544 [label="encoder.0.main.0.bias
 (24)" fillcolor=lightblue]
	1661311720544 -> 1666594550400
	1666594550400 [label=AccumulateGrad]
	1666594542576 -> 1666593960576
	1661311720464 [label="encoder.0.main.1.weight
 (24)" fillcolor=lightblue]
	1661311720464 -> 1666594542576
	1666594542576 [label=AccumulateGrad]
	1666594551072 -> 1666593960576
	1661311728064 [label="encoder.0.main.1.bias
 (24)" fillcolor=lightblue]
	1661311728064 -> 1666594551072
	1666594551072 [label=AccumulateGrad]
	1666593949392 -> 1665458956672
	1666593949392 [label=LeakyReluBackward0]
	1666593956784 -> 1666593949392
	1666593956784 [label=CudnnBatchNormBackward0]
	1666593953184 -> 1666593956784
	1666593953184 [label=ConvolutionBackward0]
	1666594552416 -> 1666593953184
	1661311715664 [label="rotor_encoder.0.0.weight
 (24, 4, 15)" fillcolor=lightblue]
	1661311715664 -> 1666594552416
	1666594552416 [label=AccumulateGrad]
	1666594551696 -> 1666593953184
	1661311724944 [label="rotor_encoder.0.0.bias
 (24)" fillcolor=lightblue]
	1661311724944 -> 1666594551696
	1666594551696 [label=AccumulateGrad]
	1666594539792 -> 1666593956784
	1661311724864 [label="rotor_encoder.0.1.weight
 (24)" fillcolor=lightblue]
	1661311724864 -> 1666594539792
	1666594539792 [label=AccumulateGrad]
	1666594542960 -> 1666593956784
	1661311715584 [label="rotor_encoder.0.1.bias
 (24)" fillcolor=lightblue]
	1661311715584 -> 1666594542960
	1666594542960 [label=AccumulateGrad]
	1666606086128 -> 1666593957648
	1661311727904 [label="encoder.1.main.0.weight
 (48, 24, 15)" fillcolor=lightblue]
	1661311727904 -> 1666606086128
	1666606086128 [label=AccumulateGrad]
	1666606078400 -> 1666593957648
	1661311727824 [label="encoder.1.main.0.bias
 (48)" fillcolor=lightblue]
	1661311727824 -> 1666606078400
	1666606078400 [label=AccumulateGrad]
	1666606072928 -> 1666593950400
	1661311720144 [label="encoder.1.main.1.weight
 (48)" fillcolor=lightblue]
	1661311720144 -> 1666606072928
	1666606072928 [label=AccumulateGrad]
	1666606073072 -> 1666593950400
	1661311720064 [label="encoder.1.main.1.bias
 (48)" fillcolor=lightblue]
	1661311720064 -> 1666606073072
	1666606073072 [label=AccumulateGrad]
	1666606087328 -> 1666593950064
	1661311719824 [label="encoder.2.main.0.weight
 (72, 48, 15)" fillcolor=lightblue]
	1661311719824 -> 1666606087328
	1666606087328 [label=AccumulateGrad]
	1666606072640 -> 1666593950064
	1661311719744 [label="encoder.2.main.0.bias
 (72)" fillcolor=lightblue]
	1661311719744 -> 1666606072640
	1666606072640 [label=AccumulateGrad]
	1666606074944 -> 1666593952656
	1661311727584 [label="encoder.2.main.1.weight
 (72)" fillcolor=lightblue]
	1661311727584 -> 1666606074944
	1666606074944 [label=AccumulateGrad]
	1666606072736 -> 1666593952656
	1661311719664 [label="encoder.2.main.1.bias
 (72)" fillcolor=lightblue]
	1661311719664 -> 1666606072736
	1666606072736 [label=AccumulateGrad]
	1666606088000 -> 1666593955152
	1661311719424 [label="encoder.3.main.0.weight
 (96, 72, 15)" fillcolor=lightblue]
	1661311719424 -> 1666606088000
	1666606088000 [label=AccumulateGrad]
	1666606072208 -> 1666593955152
	1661311719344 [label="encoder.3.main.0.bias
 (96)" fillcolor=lightblue]
	1661311719344 -> 1666606072208
	1666606072208 [label=AccumulateGrad]
	1666606072592 -> 1666593954336
	1661311719264 [label="encoder.3.main.1.weight
 (96)" fillcolor=lightblue]
	1661311719264 -> 1666606072592
	1666606072592 [label=AccumulateGrad]
	1666606072352 -> 1666593954336
	1661311727344 [label="encoder.3.main.1.bias
 (96)" fillcolor=lightblue]
	1661311727344 -> 1666606072352
	1666606072352 [label=AccumulateGrad]
	1666606084640 -> 1666593953664
	1661311727184 [label="encoder.4.main.0.weight
 (120, 96, 15)" fillcolor=lightblue]
	1661311727184 -> 1666606084640
	1666606084640 [label=AccumulateGrad]
	1666606073360 -> 1666593953664
	1661311727104 [label="encoder.4.main.0.bias
 (120)" fillcolor=lightblue]
	1661311727104 -> 1666606073360
	1666606073360 [label=AccumulateGrad]
	1666606079024 -> 1666593961632
	1661311718944 [label="encoder.4.main.1.weight
 (120)" fillcolor=lightblue]
	1661311718944 -> 1666606079024
	1666606079024 [label=AccumulateGrad]
	1666606086704 -> 1666593961632
	1661311718864 [label="encoder.4.main.1.bias
 (120)" fillcolor=lightblue]
	1661311718864 -> 1666606086704
	1666606086704 [label=AccumulateGrad]
	1666593963024 -> 1665458956912
	1666593963024 [label=AddBackward0]
	1666593953856 -> 1666593963024
	1666593953856 [label=UnsafeViewBackward0]
	1666593948048 -> 1666593953856
	1666593948048 [label=MmBackward0]
	1666593958944 -> 1666593948048
	1666593958944 [label=UnsafeViewBackward0]
	1666593951792 -> 1666593958944
	1666593951792 [label=CloneBackward0]
	1666593953616 -> 1666593951792
	1666593953616 [label=NativeDropoutBackward0]
	1666593953712 -> 1666593953616
	1666593953712 [label=CudnnRnnBackward0]
	1666593963456 -> 1666593953712
	1666593963456 [label=LeakyReluBackward0]
	1666593962448 -> 1666593963456
	1666593962448 [label=CudnnBatchNormBackward0]
	1666593948672 -> 1666593962448
	1666593948672 [label=ConvolutionBackward0]
	1666593949392 -> 1666593948672
	1666594547328 -> 1666593948672
	1661311715344 [label="rotor_encoder.1.conv1.weight
 (120, 24, 4)" fillcolor=lightblue]
	1661311715344 -> 1666594547328
	1666594547328 [label=AccumulateGrad]
	1666594550736 -> 1666593948672
	1661311715264 [label="rotor_encoder.1.conv1.bias
 (120)" fillcolor=lightblue]
	1661311715264 -> 1666594550736
	1666594550736 [label=AccumulateGrad]
	1666606078496 -> 1666593962448
	1661311715184 [label="rotor_encoder.1.batch1.weight
 (120)" fillcolor=lightblue]
	1661311715184 -> 1666606078496
	1666606078496 [label=AccumulateGrad]
	1666594548384 -> 1666593962448
	1661311724624 [label="rotor_encoder.1.batch1.bias
 (120)" fillcolor=lightblue]
	1661311724624 -> 1666594548384
	1666594548384 [label=AccumulateGrad]
	1666606085936 -> 1666593953712
	1661350737408 [label="rotor_encoder.1.lstm1.weight_ih_l0
 (512, 8192)" fillcolor=lightblue]
	1661350737408 -> 1666606085936
	1666606085936 [label=AccumulateGrad]
	1666606085072 -> 1666593953712
	1661350737328 [label="rotor_encoder.1.lstm1.weight_hh_l0
 (512, 128)" fillcolor=lightblue]
	1661350737328 -> 1666606085072
	1666606085072 [label=AccumulateGrad]
	1666606087136 -> 1666593953712
	1661311714944 [label="rotor_encoder.1.lstm1.bias_ih_l0
 (512)" fillcolor=lightblue]
	1661311714944 -> 1666606087136
	1666606087136 [label=AccumulateGrad]
	1666606073936 -> 1666593953712
	1661311724384 [label="rotor_encoder.1.lstm1.bias_hh_l0
 (512)" fillcolor=lightblue]
	1661311724384 -> 1666606073936
	1666606073936 [label=AccumulateGrad]
	1666593959856 -> 1666593948048
	1666593959856 [label=TBackward0]
	1666606072784 -> 1666593959856
	1661401090064 [label="rotor_encoder.1.linear.weight
 (1024, 128)" fillcolor=lightblue]
	1661401090064 -> 1666606072784
	1666606072784 [label=AccumulateGrad]
	1666606086800 -> 1666593963024
	1661311724464 [label="rotor_encoder.1.linear.bias
 (1024)" fillcolor=lightblue]
	1661311724464 -> 1666606086800
	1666606086800 [label=AccumulateGrad]
	1666606077344 -> 1665458954944
	1661311718624 [label="encoder.5.main.0.weight
 (144, 120, 15)" fillcolor=lightblue]
	1661311718624 -> 1666606077344
	1666606077344 [label=AccumulateGrad]
	1666606077392 -> 1665458954944
	1661311718544 [label="encoder.5.main.0.bias
 (144)" fillcolor=lightblue]
	1661311718544 -> 1666606077392
	1666606077392 [label=AccumulateGrad]
	1666606081904 -> 1665458958256
	1661311726864 [label="encoder.5.main.1.weight
 (144)" fillcolor=lightblue]
	1661311726864 -> 1666606081904
	1666606081904 [label=AccumulateGrad]
	1666606082960 -> 1665458958256
	1661311726784 [label="encoder.5.main.1.bias
 (144)" fillcolor=lightblue]
	1661311726784 -> 1666606082960
	1666606082960 [label=AccumulateGrad]
	1666606077440 -> 1665458950096
	1661311726624 [label="encoder.6.main.0.weight
 (168, 144, 15)" fillcolor=lightblue]
	1661311726624 -> 1666606077440
	1666606077440 [label=AccumulateGrad]
	1666606081136 -> 1665458950096
	1661311718224 [label="encoder.6.main.0.bias
 (168)" fillcolor=lightblue]
	1661311718224 -> 1666606081136
	1666606081136 [label=AccumulateGrad]
	1666606086848 -> 1665458958784
	1661311718144 [label="encoder.6.main.1.weight
 (168)" fillcolor=lightblue]
	1661311718144 -> 1666606086848
	1666606086848 [label=AccumulateGrad]
	1666606087424 -> 1665458958784
	1661311718064 [label="encoder.6.main.1.bias
 (168)" fillcolor=lightblue]
	1661311718064 -> 1666606087424
	1666606087424 [label=AccumulateGrad]
	1666606087520 -> 1665458960128
	1661311717824 [label="encoder.7.main.0.weight
 (192, 168, 15)" fillcolor=lightblue]
	1661311717824 -> 1666606087520
	1666606087520 [label=AccumulateGrad]
	1666606076000 -> 1665458960128
	1661311726384 [label="encoder.7.main.0.bias
 (192)" fillcolor=lightblue]
	1661311726384 -> 1666606076000
	1666606076000 [label=AccumulateGrad]
	1666606076192 -> 1665458962048
	1661311726304 [label="encoder.7.main.1.weight
 (192)" fillcolor=lightblue]
	1661311726304 -> 1666606076192
	1666606076192 [label=AccumulateGrad]
	1666606076528 -> 1665458962048
	1661311717744 [label="encoder.7.main.1.bias
 (192)" fillcolor=lightblue]
	1661311717744 -> 1666606076528
	1666606076528 [label=AccumulateGrad]
	1666606080224 -> 1665458961424
	1661311717264 [label="encoder.8.main.0.weight
 (216, 192, 15)" fillcolor=lightblue]
	1661311717264 -> 1666606080224
	1666606080224 [label=AccumulateGrad]
	1666606080272 -> 1665458961424
	1661311717184 [label="encoder.8.main.0.bias
 (216)" fillcolor=lightblue]
	1661311717184 -> 1666606080272
	1666606080272 [label=AccumulateGrad]
	1666606078544 -> 1665458960896
	1661311717104 [label="encoder.8.main.1.weight
 (216)" fillcolor=lightblue]
	1661311717104 -> 1666606078544
	1666606078544 [label=AccumulateGrad]
	1666606078352 -> 1665458960896
	1661311725904 [label="encoder.8.main.1.bias
 (216)" fillcolor=lightblue]
	1661311725904 -> 1666606078352
	1666606078352 [label=AccumulateGrad]
	1665458952400 -> 1665458961616
	1665458952400 [label=AddBackward0]
	1665458960176 -> 1665458952400
	1665458960176 [label=UnsafeViewBackward0]
	1665458955424 -> 1665458960176
	1665458955424 [label=MmBackward0]
	1665458950816 -> 1665458955424
	1665458950816 [label=UnsafeViewBackward0]
	1665458961952 -> 1665458950816
	1665458961952 [label=CloneBackward0]
	1665458958928 -> 1665458961952
	1665458958928 [label=NativeDropoutBackward0]
	1665458958544 -> 1665458958928
	1665458958544 [label=CudnnRnnBackward0]
	1665458961184 -> 1665458958544
	1665458961184 [label=LeakyReluBackward0]
	1665458961232 -> 1665458961184
	1665458961232 [label=CudnnBatchNormBackward0]
	1665458956576 -> 1665458961232
	1665458956576 [label=ConvolutionBackward0]
	1666593963024 -> 1665458956576
	1666606083440 -> 1665458956576
	1661311716544 [label="rotor_encoder.2.conv1.weight
 (216, 120, 32)" fillcolor=lightblue]
	1661311716544 -> 1666606083440
	1666606083440 [label=AccumulateGrad]
	1666606083920 -> 1665458956576
	1661311716464 [label="rotor_encoder.2.conv1.bias
 (216)" fillcolor=lightblue]
	1661311716464 -> 1666606083920
	1666606083920 [label=AccumulateGrad]
	1666606086896 -> 1665458961232
	1661311716384 [label="rotor_encoder.2.batch1.weight
 (216)" fillcolor=lightblue]
	1661311716384 -> 1666606086896
	1666606086896 [label=AccumulateGrad]
	1666606086272 -> 1665458961232
	1661311725424 [label="rotor_encoder.2.batch1.bias
 (216)" fillcolor=lightblue]
	1661311725424 -> 1666606086272
	1666606086272 [label=AccumulateGrad]
	1666606074080 -> 1665458958544
	1661311729584 [label="rotor_encoder.2.lstm1.weight_ih_l0
 (512, 64)" fillcolor=lightblue]
	1661311729584 -> 1666606074080
	1666606074080 [label=AccumulateGrad]
	1666606085120 -> 1665458958544
	1661393492128 [label="rotor_encoder.2.lstm1.weight_hh_l0
 (512, 128)" fillcolor=lightblue]
	1661393492128 -> 1666606085120
	1666606085120 [label=AccumulateGrad]
	1666606088096 -> 1665458958544
	1661400177360 [label="rotor_encoder.2.lstm1.bias_ih_l0
 (512)" fillcolor=lightblue]
	1661400177360 -> 1666606088096
	1666606088096 [label=AccumulateGrad]
	1666606086608 -> 1665458958544
	1661400177840 [label="rotor_encoder.2.lstm1.bias_hh_l0
 (512)" fillcolor=lightblue]
	1661400177840 -> 1666606086608
	1666606086608 [label=AccumulateGrad]
	1665458960560 -> 1665458955424
	1665458960560 [label=TBackward0]
	1666606083824 -> 1665458960560
	1661400178000 [label="rotor_encoder.2.linear.weight
 (64, 128)" fillcolor=lightblue]
	1661400178000 -> 1666606083824
	1666606083824 [label=AccumulateGrad]
	1666606080176 -> 1665458952400
	1661393884784 [label="rotor_encoder.2.linear.bias
 (64)" fillcolor=lightblue]
	1661393884784 -> 1666606080176
	1666606080176 [label=AccumulateGrad]
	1666606079984 -> 1665458956384
	1661311726064 [label="encoder.9.main.0.weight
 (240, 216, 15)" fillcolor=lightblue]
	1661311726064 -> 1666606079984
	1666606079984 [label=AccumulateGrad]
	1666606079456 -> 1665458956384
	1661311725984 [label="encoder.9.main.0.bias
 (240)" fillcolor=lightblue]
	1661311725984 -> 1666606079456
	1666606079456 [label=AccumulateGrad]
	1666606082144 -> 1665458957296
	1661311717024 [label="encoder.9.main.1.weight
 (240)" fillcolor=lightblue]
	1661311717024 -> 1666606082144
	1666606082144 [label=AccumulateGrad]
	1666606079072 -> 1665458957296
	1661311716944 [label="encoder.9.main.1.bias
 (240)" fillcolor=lightblue]
	1661311716944 -> 1666606079072
	1666606079072 [label=AccumulateGrad]
	1666606084976 -> 1665458960272
	1661311716704 [label="encoder.10.main.0.weight
 (264, 240, 15)" fillcolor=lightblue]
	1661311716704 -> 1666606084976
	1666606084976 [label=AccumulateGrad]
	1666606075760 -> 1665458960272
	1661311716624 [label="encoder.10.main.0.bias
 (264)" fillcolor=lightblue]
	1661311716624 -> 1666606075760
	1666606075760 [label=AccumulateGrad]
	1666606084832 -> 1665458960704
	1661311725584 [label="encoder.10.main.1.weight
 (264)" fillcolor=lightblue]
	1661311725584 -> 1666606084832
	1666606084832 [label=AccumulateGrad]
	1666606083488 -> 1665458960704
	1661311725504 [label="encoder.10.main.1.bias
 (264)" fillcolor=lightblue]
	1661311725504 -> 1666606083488
	1666606083488 [label=AccumulateGrad]
	1666606083296 -> 1665458948128
	1661311725184 [label="encoder.11.main.0.weight
 (288, 264, 15)" fillcolor=lightblue]
	1661311725184 -> 1666606083296
	1666606083296 [label=AccumulateGrad]
	1666606081664 -> 1665458948128
	1661311716064 [label="encoder.11.main.0.bias
 (288)" fillcolor=lightblue]
	1661311716064 -> 1666606081664
	1666606081664 [label=AccumulateGrad]
	1666606083104 -> 1665458948464
	1661311715984 [label="encoder.11.main.1.weight
 (288)" fillcolor=lightblue]
	1661311715984 -> 1666606083104
	1666606083104 [label=AccumulateGrad]
	1666606082192 -> 1665458948464
	1661311715904 [label="encoder.11.main.1.bias
 (288)" fillcolor=lightblue]
	1661311715904 -> 1666606082192
	1666606082192 [label=AccumulateGrad]
	1666606081088 -> 1665458948224
	1661393886064 [label="middle.0.weight
 (288, 288, 15)" fillcolor=lightblue]
	1661393886064 -> 1666606081088
	1666606081088 [label=AccumulateGrad]
	1666606082768 -> 1665458948224
	1661403053760 [label="middle.0.bias
 (288)" fillcolor=lightblue]
	1661403053760 -> 1666606082768
	1666606082768 [label=AccumulateGrad]
	1666606081280 -> 1665458948560
	1661404285264 [label="middle.1.weight
 (288)" fillcolor=lightblue]
	1661404285264 -> 1666606081280
	1666606081280 [label=AccumulateGrad]
	1666606080992 -> 1665458948560
	1661404290704 [label="middle.1.bias
 (288)" fillcolor=lightblue]
	1661404290704 -> 1666606080992
	1666606080992 [label=AccumulateGrad]
	1665458947264 -> 1665458946544
	1666606085696 -> 1665458946304
	1661404295104 [label="decoder.0.main.0.weight
 (288, 576, 5)" fillcolor=lightblue]
	1661404295104 -> 1666606085696
	1666606085696 [label=AccumulateGrad]
	1666606086656 -> 1665458946304
	1661353286672 [label="decoder.0.main.0.bias
 (288)" fillcolor=lightblue]
	1661353286672 -> 1666606086656
	1666606086656 [label=AccumulateGrad]
	1666606080080 -> 1665458948512
	1661398967504 [label="decoder.0.main.1.weight
 (288)" fillcolor=lightblue]
	1661398967504 -> 1666606080080
	1666606080080 [label=AccumulateGrad]
	1666606075712 -> 1665458948512
	1661398967344 [label="decoder.0.main.1.bias
 (288)" fillcolor=lightblue]
	1661398967344 -> 1666606075712
	1666606075712 [label=AccumulateGrad]
	1665458947072 -> 1665458951968
	1666606081568 -> 1665458952784
	1661361006496 [label="decoder.1.main.0.weight
 (264, 552, 5)" fillcolor=lightblue]
	1661361006496 -> 1666606081568
	1666606081568 [label=AccumulateGrad]
	1666606079696 -> 1665458952784
	1661393159088 [label="decoder.1.main.0.bias
 (264)" fillcolor=lightblue]
	1661393159088 -> 1666606079696
	1666606079696 [label=AccumulateGrad]
	1666606084688 -> 1665458946448
	1661400079696 [label="decoder.1.main.1.weight
 (264)" fillcolor=lightblue]
	1661400079696 -> 1666606084688
	1666606084688 [label=AccumulateGrad]
	1666606079120 -> 1665458946448
	1661404571152 [label="decoder.1.main.1.bias
 (264)" fillcolor=lightblue]
	1661404571152 -> 1666606079120
	1666606079120 [label=AccumulateGrad]
	1665458952064 -> 1665458952976
	1666606087616 -> 1665458951008
	1661404704784 [label="decoder.2.main.0.weight
 (240, 504, 5)" fillcolor=lightblue]
	1661404704784 -> 1666606087616
	1666606087616 [label=AccumulateGrad]
	1666606075952 -> 1665458951008
	1661406306336 [label="decoder.2.main.0.bias
 (240)" fillcolor=lightblue]
	1661406306336 -> 1666606075952
	1666606075952 [label=AccumulateGrad]
	1666606081952 -> 1665458952880
	1661406307856 [label="decoder.2.main.1.weight
 (240)" fillcolor=lightblue]
	1661406307856 -> 1666606081952
	1666606081952 [label=AccumulateGrad]
	1666606081376 -> 1665458952880
	1661406309056 [label="decoder.2.main.1.bias
 (240)" fillcolor=lightblue]
	1661406309056 -> 1666606081376
	1666606081376 [label=AccumulateGrad]
	1665458961616 -> 1665458960944
	1666606085888 -> 1665458955040
	1661406307776 [label="decoder.3.main.0.weight
 (216, 456, 5)" fillcolor=lightblue]
	1661406307776 -> 1666606085888
	1666606085888 [label=AccumulateGrad]
	1666606083632 -> 1665458955040
	1661406307376 [label="decoder.3.main.0.bias
 (216)" fillcolor=lightblue]
	1661406307376 -> 1666606083632
	1666606083632 [label=AccumulateGrad]
	1666606075424 -> 1665458958688
	1661406306976 [label="decoder.3.main.1.weight
 (216)" fillcolor=lightblue]
	1661406306976 -> 1666606075424
	1666606075424 [label=AccumulateGrad]
	1666606074848 -> 1665458958688
	1661406306576 [label="decoder.3.main.1.bias
 (216)" fillcolor=lightblue]
	1661406306576 -> 1666606074848
	1666606074848 [label=AccumulateGrad]
	1665458959168 -> 1665458948896
	1666606074896 -> 1665458949088
	1661406309936 [label="decoder.4.main.0.weight
 (192, 408, 5)" fillcolor=lightblue]
	1661406309936 -> 1666606074896
	1666606074896 [label=AccumulateGrad]
	1666606081712 -> 1665458949088
	1661406310016 [label="decoder.4.main.0.bias
 (192)" fillcolor=lightblue]
	1661406310016 -> 1666606081712
	1666606081712 [label=AccumulateGrad]
	1666606076816 -> 1665458953744
	1661406310096 [label="decoder.4.main.1.weight
 (192)" fillcolor=lightblue]
	1661406310096 -> 1666606076816
	1666606076816 [label=AccumulateGrad]
	1666606087952 -> 1665458953744
	1661406310176 [label="decoder.4.main.1.bias
 (192)" fillcolor=lightblue]
	1661406310176 -> 1666606087952
	1666606087952 [label=AccumulateGrad]
	1665458949280 -> 1665458953168
	1667256896672 -> 1665458958016
	1661406310576 [label="decoder.5.main.0.weight
 (168, 360, 5)" fillcolor=lightblue]
	1661406310576 -> 1667256896672
	1667256896672 [label=AccumulateGrad]
	1667256899840 -> 1665458958016
	1661406310656 [label="decoder.5.main.0.bias
 (168)" fillcolor=lightblue]
	1661406310656 -> 1667256899840
	1667256899840 [label=AccumulateGrad]
	1667256909728 -> 1665458961760
	1661406310736 [label="decoder.5.main.1.weight
 (168)" fillcolor=lightblue]
	1661406310736 -> 1667256909728
	1667256909728 [label=AccumulateGrad]
	1667256899264 -> 1665458961760
	1661406310816 [label="decoder.5.main.1.bias
 (168)" fillcolor=lightblue]
	1661406310816 -> 1667256899264
	1667256899264 [label=AccumulateGrad]
	1665458956000 -> 1665458959984
	1667318976752 -> 1665458946976
	1661406311216 [label="decoder.6.main.0.weight
 (144, 312, 5)" fillcolor=lightblue]
	1661406311216 -> 1667318976752
	1667318976752 [label=AccumulateGrad]
	1667265986768 -> 1665458946976
	1661406311296 [label="decoder.6.main.0.bias
 (144)" fillcolor=lightblue]
	1661406311296 -> 1667265986768
	1667265986768 [label=AccumulateGrad]
	1666606077872 -> 1665458953312
	1661406311376 [label="decoder.6.main.1.weight
 (144)" fillcolor=lightblue]
	1661406311376 -> 1666606077872
	1666606077872 [label=AccumulateGrad]
	1666606082432 -> 1665458953312
	1661406311456 [label="decoder.6.main.1.bias
 (144)" fillcolor=lightblue]
	1661406311456 -> 1666606082432
	1666606082432 [label=AccumulateGrad]
	1665458956912 -> 1665458955376
	1666937446432 -> 1665458950288
	1661406311856 [label="decoder.7.main.0.weight
 (120, 264, 5)" fillcolor=lightblue]
	1661406311856 -> 1666937446432
	1666937446432 [label=AccumulateGrad]
	1666937439184 -> 1665458950288
	1661406311936 [label="decoder.7.main.0.bias
 (120)" fillcolor=lightblue]
	1661406311936 -> 1666937439184
	1666937439184 [label=AccumulateGrad]
	1666937445472 -> 1665458957056
	1661406312016 [label="decoder.7.main.1.weight
 (120)" fillcolor=lightblue]
	1661406312016 -> 1666937445472
	1666937445472 [label=AccumulateGrad]
	1666937445184 -> 1665458957056
	1661406312096 [label="decoder.7.main.1.bias
 (120)" fillcolor=lightblue]
	1661406312096 -> 1666937445184
	1666937445184 [label=AccumulateGrad]
	1665458957776 -> 1665458950048
	1666937447920 -> 1665458949712
	1661406312496 [label="decoder.8.main.0.weight
 (96, 216, 5)" fillcolor=lightblue]
	1661406312496 -> 1666937447920
	1666937447920 [label=AccumulateGrad]
	1666937447296 -> 1665458949712
	1661406312576 [label="decoder.8.main.0.bias
 (96)" fillcolor=lightblue]
	1661406312576 -> 1666937447296
	1666937447296 [label=AccumulateGrad]
	1666937438800 -> 1665458951104
	1661406312656 [label="decoder.8.main.1.weight
 (96)" fillcolor=lightblue]
	1661406312656 -> 1666937438800
	1666937438800 [label=AccumulateGrad]
	1666937447200 -> 1665458951104
	1661406312736 [label="decoder.8.main.1.bias
 (96)" fillcolor=lightblue]
	1661406312736 -> 1666937447200
	1666937447200 [label=AccumulateGrad]
	1665458952640 -> 1665458952496
	1666937443648 -> 1665458946640
	1661406313136 [label="decoder.9.main.0.weight
 (72, 168, 5)" fillcolor=lightblue]
	1661406313136 -> 1666937443648
	1666937443648 [label=AccumulateGrad]
	1666937439664 -> 1665458946640
	1661406313216 [label="decoder.9.main.0.bias
 (72)" fillcolor=lightblue]
	1661406313216 -> 1666937439664
	1666937439664 [label=AccumulateGrad]
	1666937442352 -> 1665458956288
	1661406313296 [label="decoder.9.main.1.weight
 (72)" fillcolor=lightblue]
	1661406313296 -> 1666937442352
	1666937442352 [label=AccumulateGrad]
	1666937440384 -> 1665458956288
	1661406313376 [label="decoder.9.main.1.bias
 (72)" fillcolor=lightblue]
	1661406313376 -> 1666937440384
	1666937440384 [label=AccumulateGrad]
	1665458954608 -> 1665458949616
	1666937448208 -> 1665458957536
	1661406313776 [label="decoder.10.main.0.weight
 (48, 120, 5)" fillcolor=lightblue]
	1661406313776 -> 1666937448208
	1666937448208 [label=AccumulateGrad]
	1666937446288 -> 1665458957536
	1661406313856 [label="decoder.10.main.0.bias
 (48)" fillcolor=lightblue]
	1661406313856 -> 1666937446288
	1666937446288 [label=AccumulateGrad]
	1666937442496 -> 1665458954896
	1661406313936 [label="decoder.10.main.1.weight
 (48)" fillcolor=lightblue]
	1661406313936 -> 1666937442496
	1666937442496 [label=AccumulateGrad]
	1666937442448 -> 1665458954896
	1661406314016 [label="decoder.10.main.1.bias
 (48)" fillcolor=lightblue]
	1661406314016 -> 1666937442448
	1666937442448 [label=AccumulateGrad]
	1665458956672 -> 1665458959888
	1666937439280 -> 1665458951056
	1661406314416 [label="decoder.11.main.0.weight
 (24, 72, 5)" fillcolor=lightblue]
	1661406314416 -> 1666937439280
	1666937439280 [label=AccumulateGrad]
	1666937438944 -> 1665458951056
	1661406888000 [label="decoder.11.main.0.bias
 (24)" fillcolor=lightblue]
	1661406888000 -> 1666937438944
	1666937438944 [label=AccumulateGrad]
	1666937439904 -> 1665458959312
	1661406888080 [label="decoder.11.main.1.weight
 (24)" fillcolor=lightblue]
	1661406888080 -> 1666937439904
	1666937439904 [label=AccumulateGrad]
	1666937447440 -> 1665458959312
	1661406888160 [label="decoder.11.main.1.bias
 (24)" fillcolor=lightblue]
	1661406888160 -> 1666937447440
	1666937447440 [label=AccumulateGrad]
	1666937452384 -> 1665458954272
	1661406888560 [label="out.0.weight
 (1, 25, 1)" fillcolor=lightblue]
	1661406888560 -> 1666937452384
	1666937452384 [label=AccumulateGrad]
	1666937449120 -> 1665458954272
	1661406888640 [label="out.0.bias
 (1)" fillcolor=lightblue]
	1661406888640 -> 1666937449120
	1666937449120 [label=AccumulateGrad]
	1665458955280 -> 1665458185744
}
